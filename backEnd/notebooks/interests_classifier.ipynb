{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import nltk\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score ,confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_json('../data/interests.json', lines=True)\n",
    "# remove_columns_list = ['authors', 'date', 'link', 'short_description', 'headline']\n",
    "news['information'] = news[['headline', 'short_description']].apply(lambda x: ' '.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124989, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = news\n",
    "news.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.drop(news[(news['authors'] == '') & (news['short_description'] == '' )].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(news[['information', 'authors']], news['category'], test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas series into numpy array\n",
    "X_train = np.array(X_train);\n",
    "X_test = np.array(X_test);\n",
    "Y_train = np.array(Y_train);\n",
    "Y_test = np.array(Y_test);\n",
    "cleanHeadlines_train = [] #To append processed headlines\n",
    "cleanHeadlines_test = [] #To append processed headlines\n",
    "number_reviews_train = len(X_train) #Calculating the number of reviews\n",
    "number_reviews_test = len(X_test) #Calculating the number of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'ARTS' u'ARTS & CULTURE' u'BLACK VOICES' u'BUSINESS' u'COLLEGE'\n",
      " u'COMEDY' u'CRIME' u'EDUCATION' u'ENTERTAINMENT' u'FIFTY' u'GOOD NEWS'\n",
      " u'GREEN' u'HEALTHY LIVING' u'IMPACT' u'LATINO VOICES' u'MEDIA' u'PARENTS'\n",
      " u'POLITICS' u'QUEER VOICES' u'RELIGION' u'SCIENCE' u'SPORTS' u'STYLE'\n",
      " u'TASTE' u'TECH' u'THE WORLDPOST' u'TRAVEL' u'WEIRD NEWS' u'WOMEN'\n",
      " u'WORLD NEWS' u'WORLDPOST']\n",
      "(31,)\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(Y_train))\n",
    "print(np.unique(Y_train).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "lemmetizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "def get_words(headlines_list):\n",
    "    headlines = headlines_list[0]   \n",
    "    author_names = [x for x in headlines_list[1].lower().replace('and',',').replace(' ', '').split(',') if x != '']\n",
    "    headlines_only_letters = re.sub('[^a-zA-Z]', ' ', headlines)\n",
    "    words = nltk.word_tokenize(headlines_only_letters.lower())\n",
    "    stops = set(stopwords.words('english'))\n",
    "    meaningful_words = [lemmetizer.lemmatize(w) for w in words if w not in stops]\n",
    "    return ' '.join(meaningful_words + author_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,number_reviews_train):\n",
    "    cleanHeadline = get_words(X_train[i]) #Processing the data and getting words with no special characters, numbers or html tags\n",
    "    cleanHeadlines_train.append( cleanHeadline )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,number_reviews_test):\n",
    "    cleanHeadline = get_words(X_test[i]) #Processing the data and getting words with no special characters, numbers or html tags\n",
    "    cleanHeadlines_test.append( cleanHeadline )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize = sklearn.feature_extraction.text.TfidfVectorizer(analyzer = \"word\", max_features=30000)\n",
    "tfidwords_train = vectorize.fit_transform(cleanHeadlines_train)\n",
    "X_train = tfidwords_train.toarray()\n",
    "tfidwords_test = vectorize.transform(cleanHeadlines_test)\n",
    "X_test = tfidwords_test.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.77\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB(alpha=0.1)\n",
    "model.fit(X_train,Y_train)\n",
    "Y_predict = model.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test,Y_predict)*100\n",
    "print(format(accuracy, '.2f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_text\n",
    "from sklearn.pipeline import make_pipeline\n",
    "c = make_pipeline(vectorize, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.001 0.003 0.006 0.001 0.001 0.008 0.035 0.    0.033 0.    0.002 0.001\n",
      "  0.    0.002 0.001 0.011 0.002 0.208 0.059 0.007 0.001 0.003 0.001 0.\n",
      "  0.001 0.561 0.001 0.004 0.002 0.037 0.01 ]]\n"
     ]
    }
   ],
   "source": [
    "print(c.predict_proba([cleanHeadlines_test[0]]).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "explainer = LimeTextExplainer(class_names=np.unique(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document id: 5\n",
      "Predicted class = WORLD NEWS\n",
      "True class: WORLDPOST\n"
     ]
    }
   ],
   "source": [
    "idx = 5\n",
    "exp = explainer.explain_instance(cleanHeadlines_test[idx], c.predict_proba, labels=range(12))\n",
    "print('Document id: %d' % idx)\n",
    "print 'Predicted class =', model.predict(X_test[idx:idx+1]).reshape(1,-1)[0,0]\n",
    "print('True class: %s' % Y_test[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation for class COMEDY\n",
      "(u'korea', -0.004978271484924064)\n",
      "(u'north', -0.0029344702673439545)\n",
      "(u'photo', 0.0019411912382061235)\n",
      "(u'elinegordts', -0.0017156245593885218)\n",
      "(u'epic', 0.0008708329301280892)\n",
      "(u'road', 0.00026994453103323425)\n",
      "(u'trip', 0.0001661502221713568)\n",
      "()\n",
      "Explanation for class HEALTHY LIVING\n",
      "(u'korea', -0.001808779903109167)\n",
      "(u'epic', -0.00094640231898136)\n",
      "(u'trip', -0.0008101255505702401)\n",
      "(u'photo', 0.0006767684434909784)\n",
      "(u'elinegordts', -0.0005587426127365393)\n",
      "(u'north', -0.0005291523539648586)\n",
      "(u'road', 8.960136930445545e-05)\n",
      "()\n",
      "Explanation for class POLITICS\n",
      "(u'korea', -0.009455791414556144)\n",
      "(u'elinegordts', -0.004623117079857041)\n",
      "(u'epic', 0.00338054391446005)\n",
      "(u'north', -0.0015316925671250588)\n",
      "(u'trip', -0.0012865281095497)\n",
      "(u'photo', 0.0012694029335408071)\n",
      "(u'road', -0.0011319009380489815)\n",
      "()\n",
      "Explanation for class ENTERTAINMENT\n",
      "(u'epic', -0.004482552022822484)\n",
      "(u'korea', -0.0034098532937142114)\n",
      "(u'photo', -0.0015056718848523226)\n",
      "(u'north', -0.0014322971556273358)\n",
      "(u'road', 0.000870198395729596)\n",
      "(u'elinegordts', -0.000441752726657919)\n",
      "(u'trip', 0.00019867766363776111)\n",
      "()\n",
      "Explanation for class BUSINESS\n",
      "(u'korea', -0.0009727493729863137)\n",
      "(u'trip', -0.0004410644322564991)\n",
      "(u'photo', -0.00029700142440496096)\n",
      "(u'north', -0.00017996046592608196)\n",
      "(u'elinegordts', -0.00014982985093416003)\n",
      "(u'epic', 7.071205846162567e-05)\n",
      "(u'road', -5.258267724449739e-05)\n",
      "()\n",
      "Explanation for class HEALTHY LIVING\n",
      "(u'elinegordts', -0.018126426925104477)\n",
      "(u'epic', 0.015241488454247488)\n",
      "(u'road', -0.009108440225800048)\n",
      "(u'north', -0.007183566605298468)\n",
      "(u'korea', 0.0028234028535506586)\n",
      "(u'trip', -0.0016915536952012396)\n",
      "(u'photo', 0.0012985558463165926)\n",
      "()\n",
      "Explanation for class WEIRD NEWS\n",
      "(u'korea', -0.008544230160332799)\n",
      "(u'epic', -0.005278493252884446)\n",
      "(u'road', 0.002535746500464635)\n",
      "(u'north', -0.00221924845965328)\n",
      "(u'elinegordts', 0.000902806741601371)\n",
      "(u'photo', 0.0004991066333277949)\n",
      "(u'trip', 0.0004920500409939686)\n",
      "()\n",
      "Explanation for class BUSINESS\n",
      "(u'road', -0.00047975076727935803)\n",
      "(u'photo', -0.00043205230914613403)\n",
      "(u'epic', -0.0001790671911317536)\n",
      "(u'north', -0.0001716877598954552)\n",
      "(u'korea', -0.00014208256611404212)\n",
      "(u'elinegordts', -0.00012345844763701752)\n",
      "(u'trip', 9.374223303875242e-06)\n",
      "()\n",
      "Explanation for class SPORTS\n",
      "(u'elinegordts', -0.15149711328138524)\n",
      "(u'epic', 0.07172615011318158)\n",
      "(u'korea', -0.06346464299654848)\n",
      "(u'photo', 0.038126573003521416)\n",
      "(u'road', 0.025983503868517832)\n",
      "(u'north', -0.01878429604361321)\n",
      "(u'trip', -0.005279475879898651)\n",
      "()\n",
      "Explanation for class WORLD NEWS\n",
      "(u'north', -0.0014299262294771302)\n",
      "(u'korea', -0.0011015759606036635)\n",
      "(u'road', 0.0009605514329246294)\n",
      "(u'elinegordts', -0.0009187797855663224)\n",
      "(u'trip', -0.0005661196550531344)\n",
      "(u'photo', -0.00043037991185224095)\n",
      "(u'epic', 3.0818916309142014e-06)\n",
      "()\n",
      "Explanation for class POLITICS\n",
      "(u'epic', 0.004971397173439377)\n",
      "(u'korea', -0.004320260038553203)\n",
      "(u'north', -0.003861590555658699)\n",
      "(u'elinegordts', -0.003546253681797465)\n",
      "(u'photo', 0.002778861261951025)\n",
      "(u'road', -0.0024866968855500565)\n",
      "(u'trip', 0.0018431347501526158)\n",
      "()\n",
      "Explanation for class POLITICS\n",
      "(u'korea', -0.019603276222122223)\n",
      "(u'elinegordts', 0.011419351045629725)\n",
      "(u'epic', 0.00917345742599685)\n",
      "(u'road', -0.0042074669628512544)\n",
      "(u'photo', 0.0026268257164022034)\n",
      "(u'trip', -0.002341904366895597)\n",
      "(u'north', -0.002328489642205629)\n",
      "()\n",
      "Explanation for class POLITICS\n",
      "Error for 12\n",
      "Explanation for class COMEDY\n",
      "Error for 13\n",
      "Explanation for class BUSINESS\n",
      "Error for 14\n",
      "Explanation for class POLITICS\n",
      "Error for 15\n",
      "Explanation for class ARTS & CULTURE\n",
      "Error for 16\n",
      "Explanation for class GOOD NEWS\n",
      "Error for 17\n",
      "Explanation for class POLITICS\n",
      "Error for 18\n",
      "Explanation for class POLITICS\n",
      "Error for 19\n",
      "Explanation for class HEALTHY LIVING\n",
      "Error for 20\n",
      "Explanation for class COMEDY\n",
      "Error for 21\n",
      "Explanation for class POLITICS\n",
      "Error for 22\n",
      "Explanation for class CRIME\n",
      "Error for 23\n",
      "Explanation for class GREEN\n",
      "Error for 24\n",
      "Explanation for class POLITICS\n",
      "Error for 25\n",
      "Explanation for class COMEDY\n",
      "Error for 26\n",
      "Explanation for class SPORTS\n",
      "Error for 27\n",
      "Explanation for class POLITICS\n",
      "Error for 28\n",
      "Explanation for class MEDIA\n",
      "Error for 29\n",
      "Explanation for class HEALTHY LIVING\n",
      "Error for 30\n"
     ]
    }
   ],
   "source": [
    "for i in range(31):\n",
    "    try:\n",
    "        print ('Explanation for class %s' % Y_train[i])\n",
    "        print ('\\n'.join(map(str, exp.as_list(label=i))))\n",
    "        print ()\n",
    "    except:\n",
    "        print(\"Error for {}\".format(i))\n",
    "# print ('Explanation for class %s' % Y_train[28])\n",
    "# print ('\\n'.join(map(str, exp.as_list(label=28))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document id: 5\n",
      "Predicted class = WORLD NEWS\n",
      "True class: WORLDPOST\n"
     ]
    }
   ],
   "source": [
    "top_labels=2\n",
    "idx = 5\n",
    "exp = explainer.explain_instance(cleanHeadlines_test[idx], c.predict_proba, labels=[2,3,4])\n",
    "print('Document id: %d' % idx)\n",
    "print 'Predicted class =', model.predict(X_test[idx:idx+1]).reshape(1,-1)[0,0]\n",
    "print('True class: %s' % Y_test[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation for class ARTS\n",
      "Error for 0\n",
      "Explanation for class ARTS & CULTURE\n",
      "Error for 1\n",
      "Explanation for class BLACK VOICES\n",
      "(u'korea', -0.009484669830319273)\n",
      "(u'elinegordts', -0.004587156507241513)\n",
      "(u'epic', 0.003397189058609107)\n",
      "(u'north', -0.0015573514761584124)\n",
      "(u'trip', -0.0013769590973388656)\n",
      "(u'photo', 0.0011975854364029274)\n",
      "(u'road', -0.0010375610344270187)\n",
      "()\n",
      "Explanation for class BUSINESS\n",
      "(u'epic', -0.004412287523810453)\n",
      "(u'korea', -0.003361781870488386)\n",
      "(u'photo', -0.0015111316427056877)\n",
      "(u'north', -0.001396710813581932)\n",
      "(u'road', 0.0008023998630574458)\n",
      "(u'elinegordts', -0.0004698121228776304)\n",
      "(u'trip', 0.0002747928882003466)\n",
      "()\n",
      "Explanation for class COLLEGE\n",
      "(u'korea', -0.0009781660305471641)\n",
      "(u'trip', -0.00043768097513803474)\n",
      "(u'photo', -0.0002936087534524023)\n",
      "(u'north', -0.00018954936504184035)\n",
      "(u'elinegordts', -0.0001613340537480655)\n",
      "(u'epic', 7.677713541064159e-05)\n",
      "(u'road', -6.442482737635111e-05)\n",
      "()\n",
      "Explanation for class COMEDY\n",
      "Error for 5\n",
      "Explanation for class CRIME\n",
      "Error for 6\n",
      "Explanation for class EDUCATION\n",
      "Error for 7\n",
      "Explanation for class ENTERTAINMENT\n",
      "Error for 8\n",
      "Explanation for class FIFTY\n",
      "Error for 9\n",
      "Explanation for class GOOD NEWS\n",
      "Error for 10\n",
      "Explanation for class GREEN\n",
      "Error for 11\n"
     ]
    }
   ],
   "source": [
    "for i in range(12):\n",
    "    try:\n",
    "        print ('Explanation for class %s' % np.unique(Y_train)[i])\n",
    "        print ('\\n'.join(map(str, exp.as_list(label=i))))\n",
    "        print ()\n",
    "    except:\n",
    "        print(\"Error for {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'interests_classifier.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_filename = 'vectorize_interests_classifier.sav'\n",
    "pickle.dump(vectorize, open(vect_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.48\n"
     ]
    }
   ],
   "source": [
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "Y_predict = loaded_model.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test,Y_predict)*100\n",
    "print(format(accuracy, '.2f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test = np.asarray([u\"Cricket is a beatiful sport. Baseball, hockey. Dhoni is the best!!!\",\n",
    "       u'Jenna Amatulli'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanHeadlines_test1 = []\n",
    "cleanHeadline = get_words(input_test) #Processing the data and getting words with no special characters, numbers or html tags\n",
    "cleanHeadlines_test1.append( cleanHeadline )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x858 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 1 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidwords_input_test = vectorize.transform(cleanHeadlines_test1)\n",
    "tfidwords_input_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'POLITICS'], dtype='<U13')"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(tfidwords_input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(cleanHeadlines_test1[0], c.predict_proba, labels=range(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class = POLITICS\n",
      "i: BLACK VOICES\n",
      "(u'jennaamatulli', 0.032745966099185266)\n",
      "(u'cricket', 2.9523109376155798e-06)\n",
      "(u'hockey', 2.8311636531658235e-06)\n",
      "(u'baseball', 1.8942035940736854e-06)\n",
      "(u'sport', 1.881192608186352e-06)\n",
      "(u'best', 1.3817592609477815e-06)\n",
      "(u'dhoni', 8.68532935870144e-07)\n",
      "(u'beatiful', -4.4182982851245567e-07)\n",
      "i: BUSINESS\n",
      "(u'jennaamatulli', -0.004872904322553878)\n",
      "(u'cricket', -4.393313266690357e-07)\n",
      "(u'hockey', -4.2130348396447854e-07)\n",
      "(u'baseball', -2.8187511259792557e-07)\n",
      "(u'sport', -2.799389568842294e-07)\n",
      "(u'best', -2.0561862963529966e-07)\n",
      "(u'dhoni', -1.2924577899653078e-07)\n",
      "(u'beatiful', 6.574838789817101e-08)\n",
      "i: COMEDY\n",
      "(u'jennaamatulli', -0.004848473702200255)\n",
      "(u'cricket', -4.3712871070583687e-07)\n",
      "(u'hockey', -4.191912517543843e-07)\n",
      "(u'baseball', -2.804619135273674e-07)\n",
      "(u'sport', -2.7853546485512615e-07)\n",
      "(u'best', -2.0458774736383447e-07)\n",
      "(u'dhoni', -1.285977969409541e-07)\n",
      "(u'beatiful', 6.541875411168215e-08)\n",
      "i: CRIME\n",
      "(u'jennaamatulli', -0.01045205857925458)\n",
      "(u'cricket', -9.42336737619057e-07)\n",
      "(u'hockey', -9.036682033060016e-07)\n",
      "(u'baseball', -6.046035370065661e-07)\n",
      "(u'sport', -6.004506106201774e-07)\n",
      "(u'best', -4.4103840742070795e-07)\n",
      "(u'dhoni', -2.772236768406074e-07)\n",
      "(u'beatiful', 1.4102595830238866e-07)\n",
      "i: ENTERTAINMENT\n",
      "(u'jennaamatulli', -0.003885610230576569)\n",
      "(u'cricket', -3.503188621245272e-07)\n",
      "(u'hockey', -3.3594362193678915e-07)\n",
      "(u'baseball', -2.2476468831701838e-07)\n",
      "(u'sport', -2.232208151050074e-07)\n",
      "(u'best', -1.639584522949266e-07)\n",
      "(u'dhoni', -1.0305942573182171e-07)\n",
      "(u'beatiful', 5.242717520203912e-08)\n",
      "i: IMPACT\n",
      "(u'jennaamatulli', -0.004768688250944766)\n",
      "(u'cricket', -4.2993541368408766e-07)\n",
      "(u'hockey', -4.122931297389726e-07)\n",
      "(u'baseball', -2.7584669197372715e-07)\n",
      "(u'sport', -2.7395194453155574e-07)\n",
      "(u'best', -2.0122109493925516e-07)\n",
      "(u'dhoni', -1.2648161896650003e-07)\n",
      "(u'beatiful', 6.43422370181825e-08)\n",
      "i: POLITICS\n",
      "(u'jennaamatulli', 0.04791843542850972)\n",
      "(u'cricket', 4.3202304858109435e-06)\n",
      "(u'hockey', 4.142950990999876e-06)\n",
      "(u'baseball', 2.7718611915775736e-06)\n",
      "(u'sport', 2.7528217140068113e-06)\n",
      "(u'best', 2.0219816304369732e-06)\n",
      "(u'dhoni', 1.2709577503056936e-06)\n",
      "(u'beatiful', -6.465466324571883e-07)\n",
      "i: QUEER VOICES\n",
      "(u'jennaamatulli', -0.016673775645420014)\n",
      "(u'cricket', -1.503274328820141e-06)\n",
      "(u'hockey', -1.4415878714764163e-06)\n",
      "(u'baseball', -9.64501265855284e-07)\n",
      "(u'sport', -9.578762587035342e-07)\n",
      "(u'best', -7.035719710717226e-07)\n",
      "(u'dhoni', -4.4224449721449233e-07)\n",
      "(u'beatiful', 2.2497340318977095e-07)\n",
      "i: SPORTS\n",
      "(u'jennaamatulli', -0.004753437947097477)\n",
      "(u'cricket', -4.2856047673151496e-07)\n",
      "(u'hockey', -4.109746129540693e-07)\n",
      "(u'baseball', -2.749645320076629e-07)\n",
      "(u'sport', -2.7307584398247745e-07)\n",
      "(u'best', -2.0057758823953973e-07)\n",
      "(u'dhoni', -1.2607712971927297e-07)\n",
      "(u'beatiful', 6.413647002041587e-08)\n",
      "i: WEIRD NEWS\n",
      "(u'jennaamatulli', 0.027963028895326616)\n",
      "(u'cricket', 2.52109086677921e-06)\n",
      "(u'hockey', 2.4176385818364425e-06)\n",
      "(u'baseball', 1.6175326656803432e-06)\n",
      "(u'sport', 1.6064220887874968e-06)\n",
      "(u'best', 1.179936912633275e-06)\n",
      "(u'dhoni', 7.416733868456668e-07)\n",
      "(u'beatiful', -3.7729533537298655e-07)\n",
      "i: WOMEN\n",
      "(u'jennaamatulli', -0.010220283220928338)\n",
      "(u'cricket', -9.214403339712782e-07)\n",
      "(u'hockey', -8.836292779538225e-07)\n",
      "(u'baseball', -5.911963980811754e-07)\n",
      "(u'sport', -5.871355632175028e-07)\n",
      "(u'best', -4.312583402559503e-07)\n",
      "(u'dhoni', -2.7107621636275266e-07)\n",
      "(u'beatiful', 1.37898694733067e-07)\n",
      "i: WORLD NEWS\n",
      "(u'jennaamatulli', -0.0481521985240456)\n",
      "(u'cricket', -4.341306099878272e-06)\n",
      "(u'hockey', -4.163161772915783e-06)\n",
      "(u'baseball', -2.78538331196099e-06)\n",
      "(u'sport', -2.7662509530790565e-06)\n",
      "(u'best', -2.0318455727971684e-06)\n",
      "(u'dhoni', -1.2771579322483927e-06)\n",
      "(u'beatiful', 6.497007158661587e-07)\n"
     ]
    }
   ],
   "source": [
    "print 'Predicted class =', loaded_model.predict(tfidwords_input_test).reshape(1,-1)[0,0]\n",
    "for i in range(12):\n",
    "    try:\n",
    "        print(\"i: {}\".format(np.unique(Y_train)[i]))\n",
    "        print ('\\n'.join(map(str, exp.as_list(label=i))))\n",
    "#         if Y_test[i] == \"SPORTS\":\n",
    "#             print ('Explanation for class %s' % loaded_model.predict(tfidwords_input_test)[0])\n",
    "            \n",
    "#         print ()\n",
    "    except:\n",
    "        print(\"Error for {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
